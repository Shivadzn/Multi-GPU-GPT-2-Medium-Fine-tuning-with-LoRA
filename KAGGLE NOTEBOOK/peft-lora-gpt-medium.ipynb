{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Step 1: Enviornment Setup","metadata":{"id":"ybKlNRJBwpYA"}},{"cell_type":"code","source":"# ============================================================\n# Cell 1: Install dependencies\n# ============================================================\n!pip install -q accelerate peft datasets transformers wandb\nprint(\"✓ Packages installed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T05:35:18.548214Z","iopub.execute_input":"2025-10-28T05:35:18.548418Z","iopub.status.idle":"2025-10-28T05:36:27.005820Z","shell.execute_reply.started":"2025-10-28T05:35:18.548399Z","shell.execute_reply":"2025-10-28T05:36:27.004885Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m✓ Packages installed\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Step 2:  Multi GPU and pipeline configuration setup: ","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Cell 2: Create Accelerate config file\n# ============================================================\nconfig_text = \"\"\"compute_environment: LOCAL_MACHINE\ndistributed_type: MULTI_GPU\ndowncast_bf16: 'no'\ngpu_ids: all\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: fp16\nnum_machines: 1\nnum_processes: 2\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n\"\"\"\nwith open(\"accelerate_config.yaml\", \"w\") as f:\n    f.write(config_text)\nprint(\"✓ accelerate_config.yaml created\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T05:36:27.007795Z","iopub.execute_input":"2025-10-28T05:36:27.008084Z","iopub.status.idle":"2025-10-28T05:36:27.013946Z","shell.execute_reply.started":"2025-10-28T05:36:27.008061Z","shell.execute_reply":"2025-10-28T05:36:27.013111Z"}},"outputs":[{"name":"stdout","text":"✓ accelerate_config.yaml created\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Step 3: Pipeline Setup Script","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Cell 3: Create train.py\n# ============================================================\n\ntrain_script = r\"\"\"#!/usr/bin/env python3\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nos.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n\n# ADD THESE LINES TO FIX NCCL:\nos.environ['NCCL_TIMEOUT'] = '3600'           # 1 hour timeout (default: 10 min)\nos.environ['NCCL_ASYNC_ERROR_HANDLING'] = '1' # Better error handling\nos.environ['NCCL_DEBUG'] = 'WARN'             # Less verbose logging\nos.environ['NCCL_IB_DISABLE'] = '1'           # Disable InfiniBand (not on Kaggle)\nos.environ['NCCL_P2P_DISABLE'] = '1'          # Disable P2P (can help on cloud)\n\nimport time\nimport math\nimport gc\nimport warnings\nimport logging\nimport random\nimport torch\nimport numpy as np\nimport argparse\nfrom datetime import datetime\n\ntry:\n    import wandb\n    WANDB_AVAILABLE = True\nexcept:\n    wandb = None\n    WANDB_AVAILABLE = False\n\nfrom kaggle_secrets import UserSecretsClient\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling,\n    Trainer, TrainingArguments, EarlyStoppingCallback, TrainerCallback\n)\nfrom peft import LoraConfig, get_peft_model\nfrom accelerate import Accelerator\n\nwarnings.filterwarnings(\"ignore\")\nlogging.getLogger(\"transformers\").setLevel(logging.ERROR)\n\nclass TrainingConfig:\n    def __init__(self, args):\n        self.model_name = \"gpt2-medium\"\n        self.use_wikitext_2 = args.use_wikitext_2\n        self.dataset_name = \"wikitext-2-raw-v1\" if self.use_wikitext_2 else \"wikitext-103-raw-v1\"\n        self.max_length = args.max_length\n        self.lora_r = args.lora_r\n        self.lora_alpha = args.lora_alpha\n        self.lora_dropout = args.lora_dropout\n        self.learning_rate = args.learning_rate\n        self.num_epochs = args.num_epochs\n        self.per_device_batch = args.batch_size\n        self.grad_accum = args.grad_accum\n        self.weight_decay = args.weight_decay\n        self.warmup_ratio = args.warmup_ratio\n        self.scheduler_type = args.scheduler\n        self.early_stopping_patience = args.patience\n        self.output_dir = args.output_dir\n        self.save_steps = args.save_steps\n        self.eval_steps = args.eval_steps\n        self.resume_from_checkpoint = args.resume\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--use_wikitext_2\", action=\"store_true\")\n    parser.add_argument(\"--max_length\", type=int, default=512)\n    parser.add_argument(\"--lora_r\", type=int, default=16)\n    parser.add_argument(\"--lora_alpha\", type=int, default=32)\n    parser.add_argument(\"--lora_dropout\", type=float, default=0.05)\n    parser.add_argument(\"--learning_rate\", type=float, default=3e-4)\n    parser.add_argument(\"--num_epochs\", type=int, default=5)\n    parser.add_argument(\"--batch_size\", type=int, default=8)\n    parser.add_argument(\"--grad_accum\", type=int, default=8)\n    parser.add_argument(\"--weight_decay\", type=float, default=0.01)\n    parser.add_argument(\"--warmup_ratio\", type=float, default=0.05)\n    parser.add_argument(\"--scheduler\", type=str, default=\"cosine\")\n    parser.add_argument(\"--output_dir\", type=str, default=f\"./gpt2-finetuned-{datetime.now().strftime('%Y%m%d-%H%M%S')}\")\n    parser.add_argument(\"--save_steps\", type=int, default=250)\n    parser.add_argument(\"--eval_steps\", type=int, default=250)\n    parser.add_argument(\"--patience\", type=int, default=3)\n    parser.add_argument(\"--resume\", type=str, default=None)\n    return parser.parse_args()\n\ndef setup_environment():\n    accelerator = Accelerator()\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    if accelerator.is_main_process:\n        print(\"=\"*60)\n        print(\"ENVIRONMENT SETUP\")\n        print(\"=\"*60)\n        print(f\"CUDA: {torch.cuda.is_available()}\")\n        print(f\"GPUs: {torch.cuda.device_count()}\")\n        for i in range(torch.cuda.device_count()):\n            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n    \n    hf_token = None\n    wandb_api_key = None\n    \n    if accelerator.is_main_process:\n        try:\n            user_secrets = UserSecretsClient()\n            hf_token = user_secrets.get_secret(\"HF_API_TOKEN\")\n            wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n            os.environ[\"HF_TOKEN\"] = hf_token\n            os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n            \n            from huggingface_hub import login\n            login(token=hf_token, add_to_git_credential=False)\n            \n            if WANDB_AVAILABLE and wandb_api_key:\n                wandb.login(key=wandb_api_key)\n                print(\"✓ Logged in to HF & W&B\")\n        except Exception as e:\n            print(f\"Warning: {e}\")\n    \n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n    seed = 42\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    \n    return hf_token, wandb_api_key, accelerator\n\ndef load_model_and_tokenizer(accelerator, model_name=\"gpt2-medium\"):\n    if accelerator.is_main_process:\n        print(f\"\\n{'='*60}\\nLOADING MODEL: {model_name}\\n{'='*60}\")\n    \n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    model = AutoModelForCausalLM.from_pretrained(\n        model_name, torch_dtype=torch.float16, low_cpu_mem_usage=True\n    )\n    \n    if accelerator.is_main_process:\n        print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    \n    return model, tokenizer\n\ndef setup_lora(model, config, accelerator):\n    if accelerator.is_main_process:\n        print(f\"\\n{'='*60}\\nSETTING UP LORA\\n{'='*60}\")\n    \n    model.gradient_checkpointing_enable()\n    lora_config = LoraConfig(\n        r=config.lora_r, lora_alpha=config.lora_alpha,\n        target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],\n        lora_dropout=config.lora_dropout, use_rslora=True,\n        bias=\"none\", task_type=\"CAUSAL_LM\"\n    )\n    model = get_peft_model(model, lora_config)\n    \n    if accelerator.is_main_process:\n        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n        total = sum(p.numel() for p in model.parameters())\n        print(f\"Trainable: {trainable:,}/{total:,} ({100*trainable/total:.2f}%)\")\n        print(f\"LoRA: r={config.lora_r}, alpha={config.lora_alpha}, dropout={config.lora_dropout}\")\n    \n    return model\n\ndef setup_dataset(tokenizer, config, accelerator):\n    if accelerator.is_main_process:\n        print(f\"\\n{'='*60}\\nLOADING DATASET: {config.dataset_name}\\n{'='*60}\")\n    \n    dataset = load_dataset(\"wikitext\", config.dataset_name)\n    dataset = dataset.filter(lambda x: len(x[\"text\"].strip()) > 0)\n    \n    def tokenize_fn(examples):\n        return tokenizer(examples[\"text\"], truncation=True, max_length=config.max_length, padding=False)\n    \n    num_workers = min(os.cpu_count() or 4, 8)\n    train_ds = dataset[\"train\"].map(tokenize_fn, batched=True, num_proc=num_workers, remove_columns=[\"text\"])\n    val_ds = dataset[\"validation\"].map(tokenize_fn, batched=True, num_proc=num_workers, remove_columns=[\"text\"])\n    test_ds = dataset[\"test\"].map(tokenize_fn, batched=True, num_proc=num_workers, remove_columns=[\"text\"])\n    \n    if accelerator.is_main_process:\n        print(f\"Train: {len(train_ds):,} | Val: {len(val_ds):,} | Test: {len(test_ds):,}\")\n    \n    return train_ds, val_ds, test_ds\n\ndef get_training_args(config, accelerator):\n    world_size = accelerator.num_processes\n    effective_batch = config.per_device_batch * config.grad_accum * world_size\n    \n    if accelerator.is_main_process:\n        print(f\"\\n{'='*60}\\nTRAINING CONFIG\\n{'='*60}\")\n        print(f\"Per-device batch: {config.per_device_batch}\")\n        print(f\"Gradient accumulation: {config.grad_accum}\")\n        print(f\"GPUs: {world_size}\")\n        print(f\"Effective batch: {effective_batch}\")\n        print(f\"Learning rate: {config.learning_rate}\")\n    \n    return TrainingArguments(\n        output_dir=config.output_dir,\n        num_train_epochs=config.num_epochs,\n        per_device_train_batch_size=config.per_device_batch,\n        per_device_eval_batch_size=config.per_device_batch*2,\n        gradient_accumulation_steps=config.grad_accum,\n        learning_rate=config.learning_rate,\n        weight_decay=config.weight_decay,\n        max_grad_norm=1.0,\n        lr_scheduler_type=config.scheduler_type,\n        warmup_ratio=config.warmup_ratio,\n        optim=\"adamw_torch_fused\",\n        fp16=True,\n        fp16_full_eval=True,\n        gradient_checkpointing=True,\n        ddp_find_unused_parameters=False,\n        dataloader_num_workers=4,\n        dataloader_pin_memory=True,\n        save_strategy=\"steps\",\n        save_steps=config.save_steps,\n        save_total_limit=3,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_loss\",\n        eval_strategy=\"steps\",\n        eval_steps=config.eval_steps,\n        eval_accumulation_steps=2,\n        logging_steps=50,\n        report_to=\"wandb\" if (accelerator.is_main_process and WANDB_AVAILABLE) else \"none\",\n        run_name=f\"gpt2-lora-r{config.lora_r}\",\n        push_to_hub=False,\n    )\n\nclass SmartLoggingCallback(TrainerCallback):\n    def __init__(self, accelerator):\n        self.accelerator = accelerator\n        self.start_time = None\n        self.best_ppl = float('inf')\n    \n    def on_train_begin(self, args, state, control, **kwargs):\n        self.start_time = time.time()\n        if self.accelerator.is_main_process:\n            print(f\"\\n{'='*60}\\n⏱️ TRAINING STARTED\\n{'='*60}\")\n    \n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if self.accelerator.is_main_process and logs:\n            if 'eval_loss' in logs:\n                ppl = math.exp(min(logs['eval_loss'], 10))\n                if ppl < self.best_ppl:\n                    self.best_ppl = ppl\n                    print(f\"🎯 New Best PPL: {ppl:.2f}\")\n    \n    def on_train_end(self, args, state, control, **kwargs):\n        if self.accelerator.is_main_process and self.start_time:\n            hours = (time.time() - self.start_time) / 3600\n            print(f\"\\n{'='*60}\\n✓ DONE in {hours:.2f}h | Best PPL: {self.best_ppl:.2f}\\n{'='*60}\")\n\ndef main():\n    args = parse_args()\n    config = TrainingConfig(args)\n    hf_token, wandb_key, accelerator = setup_environment()\n    \n    model, tokenizer = load_model_and_tokenizer(accelerator)\n    model = setup_lora(model, config, accelerator)\n    train_ds, val_ds, test_ds = setup_dataset(tokenizer, config, accelerator)\n    training_args = get_training_args(config, accelerator)\n    \n    if accelerator.is_main_process and WANDB_AVAILABLE:\n        dataset_short = \"wt2\" if config.use_wikitext_2 else \"wt103\"\n        os.environ[\"WANDB_PROJECT\"] = \"gpt2-smart-finetune\"\n        os.environ[\"WANDB_NAME\"] = f\"gpt2-r{config.lora_r}-{dataset_short}\"\n    \n    trainer = Trainer(\n        model=model, args=training_args,\n        train_dataset=train_ds, eval_dataset=val_ds,\n        tokenizer=tokenizer,\n        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n        callbacks=[\n            EarlyStoppingCallback(config.early_stopping_patience),\n            SmartLoggingCallback(accelerator)\n        ],\n    )\n    \n    try:\n        trainer.train(resume_from_checkpoint=config.resume_from_checkpoint)\n        \n        if accelerator.is_main_process:\n            test_results = trainer.evaluate(test_ds)\n            ppl = math.exp(test_results[\"eval_loss\"])\n            print(f\"\\n{'='*60}\\nFINAL: Loss={test_results['eval_loss']:.4f}, PPL={ppl:.2f}\\n{'='*60}\")\n            \n            # Save model\n            final_dir = f\"{config.output_dir}/final_model\"\n            trainer.model.save_pretrained(final_dir)\n            tokenizer.save_pretrained(final_dir)\n            print(f\"✓ Model saved to: {final_dir}\")\n            \n            # Next steps\n            if config.use_wikitext_2:\n                if ppl < 20:\n                    print(\"\\n✓ Good! Now try WikiText-103 for production\")\n                else:\n                    print(\"\\n💡 Try: --learning_rate 5e-4 or --lora_r 32\")\n            else:\n                if ppl < 18:\n                    print(\"\\n🎉 Excellent! Production ready\")\n                else:\n                    print(\"\\n💡 Consider more training or tune hyperparameters\")\n                    \n    except KeyboardInterrupt:\n        print(\"\\n⚠️ Interrupted - saving...\")\n        trainer.save_model(f\"{config.output_dir}/interrupted\")\n\nif __name__ == \"__main__\":\n    main()\n\"\"\"\n\nwith open(\"train.py\", \"w\") as f:\n    f.write(train_script)\n\nprint(\"✓ train.py created successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T05:50:52.647450Z","iopub.execute_input":"2025-10-28T05:50:52.648051Z","iopub.status.idle":"2025-10-28T05:50:52.658500Z","shell.execute_reply.started":"2025-10-28T05:50:52.648027Z","shell.execute_reply":"2025-10-28T05:50:52.657870Z"}},"outputs":[{"name":"stdout","text":"✓ train.py created successfully\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Clear occupied memory of gpu:\nimport torch\ntorch.cuda.empty_cache()\nimport gc\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T05:51:02.955548Z","iopub.execute_input":"2025-10-28T05:51:02.955829Z","iopub.status.idle":"2025-10-28T05:51:06.576367Z","shell.execute_reply.started":"2025-10-28T05:51:02.955806Z","shell.execute_reply":"2025-10-28T05:51:06.575596Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"30"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"## Part 4: Launch Training","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Cell 4: Verify and Launch Training\n# ============================================================\n\nimport os\nimport torch\n\nprint(\"Checking setup...\")\nprint(f\"1. train.py exists: {os.path.exists('train.py')}\")\nprint(f\"2. config exists: {os.path.exists('accelerate_config.yaml')}\")\nprint(f\"3. GPUs available: {torch.cuda.device_count()}\")\nprint(\"\\n✓ Ready to start training!\")\nprint(\"\\n\" + \"=\"*60)\nprint(\"LAUNCHING: WikiText-2 Baseline (~2 hours)\")\nprint(\"Expected PPL: 18-21\")\nprint(\"=\"*60 + \"\\n\")\n\n# Launch with WikiText-2 (fast experimentation)\n!accelerate launch --config_file accelerate_config.yaml train.py \\\n    --use_wikitext_2 \\\n    --lora_r 16 \\\n    --lora_alpha 32 \\\n    --learning_rate 3e-4 \\\n    --num_epochs 5 \\\n    --batch_size 16 \\\n    --grad_accum 4 \\\n    --save_steps 250 \\\n    --eval_steps 250","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T05:51:27.686650Z","iopub.execute_input":"2025-10-28T05:51:27.687057Z","iopub.status.idle":"2025-10-28T07:52:11.494055Z","shell.execute_reply.started":"2025-10-28T05:51:27.687035Z","shell.execute_reply":"2025-10-28T07:52:11.493186Z"}},"outputs":[{"name":"stdout","text":"Checking setup...\n1. train.py exists: True\n2. config exists: True\n3. GPUs available: 2\n\n✓ Ready to start training!\n\n============================================================\nLAUNCHING: WikiText-2 Baseline (~2 hours)\nExpected PPL: 18-21\n============================================================\n\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761630709.506758     105 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761630709.506734     106 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761630709.571023     105 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nE0000 00:00:1761630709.571033     106 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n[W1028 05:52:06.977212173 Utils.hpp:136] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n[W1028 05:52:06.977212848 Utils.hpp:136] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n============================================================\nENVIRONMENT SETUP\n============================================================\nCUDA: True\nGPUs: 2\nGPU 0: Tesla T4\nGPU 1: Tesla T4\nNote: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\ntokenizer_config.json: 100%|██████████████████| 26.0/26.0 [00:00<00:00, 198kB/s]\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshiva_99\u001b[0m (\u001b[33mshiva_99-maharshi-dayanand-university-rohtak\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n✓ Logged in to HF & W&B\n\n============================================================\nLOADING MODEL: gpt2-medium\n============================================================\nconfig.json: 100%|█████████████████████████████| 718/718 [00:00<00:00, 5.81MB/s]\nvocab.json: 100%|██████████████████████████| 1.04M/1.04M [00:00<00:00, 9.58MB/s]\nmerges.txt: 100%|████████████████████████████| 456k/456k [00:00<00:00, 10.5MB/s]\ntokenizer.json: 100%|██████████████████████| 1.36M/1.36M [00:00<00:00, 15.4MB/s]\nmodel.safetensors: 100%|████████████████████| 1.52G/1.52G [00:05<00:00, 282MB/s]\ngeneration_config.json: 100%|███████████████████| 124/124 [00:00<00:00, 673kB/s]\nParameters: 354,823,168\n\n============================================================\nSETTING UP LORA\n============================================================\nTrainable: 6,291,456/361,114,624 (1.74%)\nLoRA: r=16, alpha=32, dropout=0.05\n\n============================================================\nLOADING DATASET: wikitext-2-raw-v1\n============================================================\nREADME.md: 10.5kB [00:00, 29.6MB/s]\nwikitext-2-raw-v1/test-00000-of-00001.pa(…): 100%|█| 733k/733k [00:00<00:00, 2.1\nwikitext-2-raw-v1/train-00000-of-00001.p(…): 100%|█| 6.36M/6.36M [00:00<00:00, 1\nwikitext-2-raw-v1/validation-00000-of-00(…): 100%|█| 657k/657k [00:00<00:00, 2.0\nGenerating test split: 100%|██████| 4358/4358 [00:00<00:00, 83320.92 examples/s]\nGenerating train split: 100%|██| 36718/36718 [00:00<00:00, 685277.19 examples/s]\nGenerating validation split: 100%|█| 3760/3760 [00:00<00:00, 554345.78 examples/\nFilter: 100%|███████████████████████| 4358/4358 [00:04<00:00, 879.48 examples/s]\nFilter: 100%|███████████████████████| 4358/4358 [00:05<00:00, 862.36 examples/s]\nFilter: 100%|██████████████████| 36718/36718 [00:00<00:00, 187921.61 examples/s]\nFilter: 100%|█████████████████████| 3760/3760 [00:00<00:00, 47077.91 examples/s]\nFilter: 100%|██████████████████| 36718/36718 [00:00<00:00, 177744.60 examples/s]\nMap (num_proc=4): 100%|██████████| 23767/23767 [00:08<00:00, 2967.90 examples/s]\nMap (num_proc=4): 100%|██████████| 23767/23767 [00:08<00:00, 2928.26 examples/s]\nMap (num_proc=4): 100%|████████████| 2461/2461 [00:01<00:00, 1655.25 examples/s]\nMap (num_proc=4): 100%|████████████| 2461/2461 [00:01<00:00, 1598.13 examples/s]\nMap (num_proc=4): 100%|████████████| 2891/2891 [00:01<00:00, 1644.38 examples/s]\nMap (num_proc=4):  75%|████████▉   | 2168/2891 [00:01<00:00, 1539.80 examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\nMap (num_proc=4): 100%|████████████| 2891/2891 [00:01<00:00, 1556.49 examples/s]\nTrain: 23,767 | Val: 2,461 | Test: 2,891\n\n============================================================\nTRAINING CONFIG\n============================================================\nPer-device batch: 16\nGradient accumulation: 4\nGPUs: 2\nEffective batch: 128\nLearning rate: 0.0003\nNCCL version 2.21.5+cuda12.4\n[rank1]:[W1028 05:52:41.688049853 Utils.hpp:111] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n[rank0]:[W1028 05:52:41.688245286 Utils.hpp:111] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to \u001b[34m\u001b[4mhttps://wandb.me/wandb-init\u001b[0m.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.20.1\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20251028_055241-8dqssswn\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgpt2-lora-r16\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/shiva_99-maharshi-dayanand-university-rohtak/gpt2-smart-finetune\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/shiva_99-maharshi-dayanand-university-rohtak/gpt2-smart-finetune/runs/8dqssswn\u001b[0m\n\n============================================================\n⏱️ TRAINING STARTED\n============================================================\n`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n{'loss': 3.4266, 'grad_norm': 0.375161349773407, 'learning_rate': 0.00029999620250019296, 'epoch': 0.2691790040376851}\n{'loss': 3.1084, 'grad_norm': 0.3222482204437256, 'learning_rate': 0.0002974401932923434, 'epoch': 0.5383580080753702}\n{'loss': 3.0748, 'grad_norm': 0.3670180141925812, 'learning_rate': 0.0002902305885868527, 'epoch': 0.8075370121130552}\n{'loss': 3.0336, 'grad_norm': 0.3346036374568939, 'learning_rate': 0.0002785949422362223, 'epoch': 1.0753701211305517}\n{'loss': 3.0089, 'grad_norm': 0.34160029888153076, 'learning_rate': 0.00026290050546456115, 'epoch': 1.3445491251682369}\n🎯 New Best PPL: 20.82\n{'eval_loss': 3.035845994949341, 'eval_runtime': 45.0469, 'eval_samples_per_second': 54.632, 'eval_steps_per_second': 0.866, 'epoch': 1.3445491251682369}\n{'loss': 2.9974, 'grad_norm': 0.32614269852638245, 'learning_rate': 0.00024364263546496366, 'epoch': 1.613728129205922}\n{'loss': 2.9802, 'grad_norm': 0.33632907271385193, 'learning_rate': 0.0002214291606397339, 'epoch': 1.8829071332436071}\n{'loss': 2.9432, 'grad_norm': 0.33555737137794495, 'learning_rate': 0.00019696119595708603, 'epoch': 2.1507402422611035}\n{'loss': 2.9274, 'grad_norm': 0.3619914650917053, 'learning_rate': 0.0001710110139414995, 'epoch': 2.4199192462987886}\n{'loss': 2.9213, 'grad_norm': 0.3930057883262634, 'learning_rate': 0.00014439766974675623, 'epoch': 2.6890982503364738}\n🎯 New Best PPL: 20.72\n{'eval_loss': 3.0311944484710693, 'eval_runtime': 44.8175, 'eval_samples_per_second': 54.912, 'eval_steps_per_second': 0.87, 'epoch': 2.6890982503364738}\n{'loss': 2.9254, 'grad_norm': 0.3837960958480835, 'learning_rate': 0.00011796114964767264, 'epoch': 2.958277254374159}\n{'loss': 2.8891, 'grad_norm': 0.3657948970794678, 'learning_rate': 9.253585889127956e-05, 'epoch': 3.2261103633916552}\n{'loss': 2.8775, 'grad_norm': 0.3783886134624481, 'learning_rate': 6.892428569973754e-05, 'epoch': 3.4952893674293404}\n{'loss': 2.8754, 'grad_norm': 0.39271751046180725, 'learning_rate': 4.787167265746529e-05, 'epoch': 3.7644683714670255}\n{'loss': 2.8685, 'grad_norm': 0.3550795614719391, 'learning_rate': 3.004249491929961e-05, 'epoch': 4.032301480484522}\n{'eval_loss': 3.031768560409546, 'eval_runtime': 44.9244, 'eval_samples_per_second': 54.781, 'eval_steps_per_second': 0.868, 'epoch': 4.032301480484522}\n{'loss': 2.8435, 'grad_norm': 0.38916313648223877, 'learning_rate': 1.599948764853796e-05, 'epoch': 4.301480484522207}\n{'loss': 2.858, 'grad_norm': 0.3778211176395416, 'learning_rate': 6.185884633398319e-06, 'epoch': 4.570659488559892}\n{'loss': 2.858, 'grad_norm': 0.36865052580833435, 'learning_rate': 9.11428677298881e-07, 'epoch': 4.839838492597577}\n{'train_runtime': 6566.6305, 'train_samples_per_second': 18.097, 'train_steps_per_second': 0.142, 'train_loss': 2.9640738128333965, 'epoch': 5.0}\n\n============================================================\n✓ DONE in 1.82h | Best PPL: 20.72\n============================================================\n[rank0]:[E1028 07:52:09.160077376 ProcessGroupNCCL.cpp:629] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3888, OpType=_ALLGATHER_BASE, NumelIn=32, NumelOut=64, Timeout(ms)=600000) ran for 600000 milliseconds before timing out.\n[rank0]:[E1028 07:52:09.175590055 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 3888 PG status: last enqueued work: 3888, last completed work: 3887\n[rank0]:[E1028 07:52:09.175622536 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.\n[rank0]:[E1028 07:52:10.060325602 ProcessGroupNCCL.cpp:681] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.\n[rank0]:[E1028 07:52:10.060355352 ProcessGroupNCCL.cpp:695] [Rank 0] To avoid data inconsistency, we are taking the entire process down.\n[rank0]:[E1028 07:52:10.066849551 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3888, OpType=_ALLGATHER_BASE, NumelIn=32, NumelOut=64, Timeout(ms)=600000) ran for 600000 milliseconds before timing out.\nException raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7d8e5eb6c1b6 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)\nframe #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7d8e0ce1bc74 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\nframe #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7d8e0ce1d7d0 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\nframe #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7d8e0ce1e6ed in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\nframe #4: <unknown function> + 0x145c0 (0x7d8e5f0035c0 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch.so)\nframe #5: <unknown function> + 0x94ac3 (0x7d8e60280ac3 in /lib/x86_64-linux-gnu/libc.so.6)\nframe #6: clone + 0x44 (0x7d8e60311a04 in /lib/x86_64-linux-gnu/libc.so.6)\n\nterminate called after throwing an instance of 'c10::DistBackendError'\n  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3888, OpType=_ALLGATHER_BASE, NumelIn=32, NumelOut=64, Timeout(ms)=600000) ran for 600000 milliseconds before timing out.\nException raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:632 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7d8e5eb6c1b6 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)\nframe #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x2b4 (0x7d8e0ce1bc74 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\nframe #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x890 (0x7d8e0ce1d7d0 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\nframe #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7d8e0ce1e6ed in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\nframe #4: <unknown function> + 0x145c0 (0x7d8e5f0035c0 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch.so)\nframe #5: <unknown function> + 0x94ac3 (0x7d8e60280ac3 in /lib/x86_64-linux-gnu/libc.so.6)\nframe #6: clone + 0x44 (0x7d8e60311a04 in /lib/x86_64-linux-gnu/libc.so.6)\n\nException raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7d8e5eb6c1b6 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0xe5c6fc (0x7d8e0ca796fc in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\nframe #2: <unknown function> + 0x145c0 (0x7d8e5f0035c0 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch.so)\nframe #3: <unknown function> + 0x94ac3 (0x7d8e60280ac3 in /lib/x86_64-linux-gnu/libc.so.6)\nframe #4: clone + 0x44 (0x7d8e60311a04 in /lib/x86_64-linux-gnu/libc.so.6)\n\nE1028 07:52:10.241000 97 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: -6) local_rank: 0 (pid: 105) of binary: /usr/bin/python3\nTraceback (most recent call last):\n  File \"/usr/local/bin/accelerate\", line 10, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/accelerate_cli.py\", line 50, in main\n    args.func(args)\n  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/launch.py\", line 1190, in launch_command\n    multi_gpu_launcher(args)\n  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/launch.py\", line 815, in multi_gpu_launcher\n    distrib_run.run(args)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 909, in run\n    elastic_launch(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n====================================================\ntrain.py FAILED\n----------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n----------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-10-28_07:52:10\n  host      : 7b0630353fb8\n  rank      : 0 (local_rank: 0)\n  exitcode  : -6 (pid: 105)\n  error_file: <N/A>\n  traceback : Signal 6 (SIGABRT) received by PID 105\n====================================================\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import glob\nruns = sorted(glob.glob(\"./gpt2-finetuned-*\"))\nif runs:\n    print(f\"Latest: {runs[-1]}\")\n    print(f\"Model at: {runs[-1]}/final_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T07:52:11.495640Z","iopub.execute_input":"2025-10-28T07:52:11.495917Z","iopub.status.idle":"2025-10-28T07:52:11.502181Z","shell.execute_reply.started":"2025-10-28T07:52:11.495891Z","shell.execute_reply":"2025-10-28T07:52:11.501337Z"}},"outputs":[{"name":"stdout","text":"Latest: ./gpt2-finetuned-20251028-055206\nModel at: ./gpt2-finetuned-20251028-055206/final_model\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nimport glob\n\n# Find your training run\nruns = sorted(glob.glob(\"./gpt2-finetuned-*\"))\nif runs:\n    latest = runs[-1]\n    print(f\"✓ Found training run: {latest}\")\n    \n    # Check for saved model\n    final_model = f\"{latest}/final_model\"\n    if os.path.exists(final_model):\n        print(f\"✓ Model saved at: {final_model}\")\n    else:\n        # Check for checkpoints\n        checkpoints = glob.glob(f\"{latest}/checkpoint-*\")\n        if checkpoints:\n            best_checkpoint = sorted(checkpoints)[-1]\n            print(f\"✓ Best checkpoint: {best_checkpoint}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T07:57:58.292459Z","iopub.execute_input":"2025-10-28T07:57:58.292781Z","iopub.status.idle":"2025-10-28T07:57:58.299424Z","shell.execute_reply.started":"2025-10-28T07:57:58.292753Z","shell.execute_reply":"2025-10-28T07:57:58.298833Z"}},"outputs":[{"name":"stdout","text":"✓ Found training run: ./gpt2-finetuned-20251028-055206\n✓ Best checkpoint: ./gpt2-finetuned-20251028-055206/checkpoint-930\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import os\n\n# Check latest run\nlatest_run = sorted([d for d in os.listdir('.') if d.startswith('gpt2-finetuned')])[-1]\nprint(f\"Latest run: {latest_run}\")\n\n# Check contents\nprint(\"\\nContents:\")\nfor item in os.listdir(latest_run):\n    print(f\"  - {item}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T07:58:07.191859Z","iopub.execute_input":"2025-10-28T07:58:07.192146Z","iopub.status.idle":"2025-10-28T07:58:07.197648Z","shell.execute_reply.started":"2025-10-28T07:58:07.192124Z","shell.execute_reply":"2025-10-28T07:58:07.197063Z"}},"outputs":[{"name":"stdout","text":"Latest run: gpt2-finetuned-20251028-055206\n\nContents:\n  - checkpoint-930\n  - checkpoint-500\n  - checkpoint-750\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\nimport torch\n\n# Load model\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"gpt2-medium\",\n    torch_dtype=torch.float16\n).to(\"cuda\")\n\n# Find your best checkpoint\nimport glob\ncheckpoints = sorted(glob.glob(\"./gpt2-finetuned-*/checkpoint-*\"))\nbest_checkpoint = checkpoints[-1] if checkpoints else \"./gpt2-finetuned-*/final_model\"\n\nprint(f\"Loading from: {best_checkpoint}\")\n\nmodel = PeftModel.from_pretrained(base_model, best_checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n\n# Test generation\nprompt = \"The history of artificial intelligence began\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\noutputs = model.generate(\n    **inputs,\n    max_length=100,\n    temperature=0.8,\n    top_p=0.9,\n    do_sample=True\n)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"GENERATED TEXT:\")\nprint(\"=\"*60)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T07:59:44.585923Z","iopub.execute_input":"2025-10-28T07:59:44.586772Z","iopub.status.idle":"2025-10-28T07:59:50.906473Z","shell.execute_reply.started":"2025-10-28T07:59:44.586734Z","shell.execute_reply":"2025-10-28T07:59:50.905634Z"}},"outputs":[{"name":"stdout","text":"Loading from: ./gpt2-finetuned-20251028-055206/checkpoint-930\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nGENERATED TEXT:\n============================================================\nThe history of artificial intelligence began in the 1980s , when a team of researchers from the University of California , Berkeley , created a machine learning algorithm , called ImageNet , that could process images and automatically make sense of them . That work led to the development of neural networks — computer networks that are able to process data and make predictions — which are now used in many fields of science and technology . The computer science field is famous for its success in creating computers that can learn , as well as for the\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Pushing the best model on Huggingface Hub","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Cell 5: Push Model to HuggingFace Hub\n# ============================================================\n\nfrom huggingface_hub import HfApi, create_repo\nfrom transformers import AutoTokenizer\nimport os\nimport glob\n\n# 1. Setup\nHF_USERNAME = \"shiva9876\"  # Your HF username\nMODEL_NAME = \"gpt2-medium-wikitext2-lora\"\nREPO_ID = f\"{HF_USERNAME}/{MODEL_NAME}\"\n\n# 2. Find your trained model\nruns = sorted(glob.glob(\"./gpt2-finetuned-*\"))\nif not runs:\n    print(\"❌ No training runs found!\")\nelse:\n    latest_run = runs[-1]\n    print(f\"✓ Found: {latest_run}\")\n    \n    # Check for model\n    final_model = f\"{latest_run}/final_model\"\n    checkpoint_dirs = sorted(glob.glob(f\"{latest_run}/checkpoint-*\"))\n    \n    if os.path.exists(final_model):\n        model_path = final_model\n    elif checkpoint_dirs:\n        model_path = checkpoint_dirs[-1]  # Use latest checkpoint\n    else:\n        print(\"❌ No model found!\")\n        model_path = None\n    \n    if model_path:\n        print(f\"✓ Model path: {model_path}\")\n        \n        # 3. Create repository on HuggingFace\n        from kaggle_secrets import UserSecretsClient\n        user_secrets = UserSecretsClient()\n        hf_token = user_secrets.get_secret(\"HF_API_TOKEN\")\n        \n        api = HfApi()\n        \n        try:\n            # Create repo (will skip if exists)\n            create_repo(\n                repo_id=REPO_ID,\n                token=hf_token,\n                private=False,  # Set True for private repo\n                exist_ok=True\n            )\n            print(f\"✓ Repository created: https://huggingface.co/{REPO_ID}\")\n        except Exception as e:\n            print(f\"⚠️ Repo creation: {e}\")\n        \n        # 4. Upload model files\n        print(\"\\nUploading model files...\")\n        api.upload_folder(\n            folder_path=model_path,\n            repo_id=REPO_ID,\n            token=hf_token,\n            commit_message=\"Upload fine-tuned GPT-2 Medium with LoRA\"\n        )\n        \n        print(f\"\\n{'='*60}\")\n        print(\"🎉 MODEL UPLOADED SUCCESSFULLY!\")\n        print(f\"{'='*60}\")\n        print(f\"View at: https://huggingface.co/{REPO_ID}\")\n        print(f\"\\nLoad anywhere with:\")\n        print(f\"  from peft import PeftModel\")\n        print(f\"  model = PeftModel.from_pretrained(\")\n        print(f\"      'gpt2-medium',\")\n        print(f\"      '{REPO_ID}'\")\n        print(f\"  )\")\n        print(f\"{'='*60}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T07:59:15.995248Z","iopub.execute_input":"2025-10-28T07:59:15.995611Z","iopub.status.idle":"2025-10-28T07:59:21.073407Z","shell.execute_reply.started":"2025-10-28T07:59:15.995586Z","shell.execute_reply":"2025-10-28T07:59:21.072538Z"}},"outputs":[{"name":"stdout","text":"✓ Found: ./gpt2-finetuned-20251028-055206\n✓ Model path: ./gpt2-finetuned-20251028-055206/checkpoint-930\n✓ Repository created: https://huggingface.co/shiva9876/gpt2-medium-wikitext2-lora\n\nUploading model files...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Uploading...:   0%|          | 0.00/75.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08b0b44bc6a24a12848b0e0ed2a7eadf"}},"metadata":{}},{"name":"stdout","text":"\n============================================================\n🎉 MODEL UPLOADED SUCCESSFULLY!\n============================================================\nView at: https://huggingface.co/shiva9876/gpt2-medium-wikitext2-lora\n\nLoad anywhere with:\n  from peft import PeftModel\n  model = PeftModel.from_pretrained(\n      'gpt2-medium',\n      'shiva9876/gpt2-medium-wikitext2-lora'\n  )\n============================================================\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Adding README file","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Cell 6: Create Model Card\n# ============================================================\n\nmodel_card = f\"\"\"---\nlanguage: en\nlicense: mit\ntags:\n  - text-generation\n  - gpt2\n  - lora\n  - peft\ndatasets:\n  - wikitext-2-raw-v1\nmetrics:\n  - perplexity\nmodel-index:\n- name: {MODEL_NAME}\n  results:\n  - task:\n      type: text-generation\n    dataset:\n      name: WikiText-2\n      type: wikitext-2-raw-v1\n    metrics:\n    - type: perplexity\n      value: 20.73\n      name: Validation Perplexity\n---\n\n# GPT-2 Medium Fine-tuned on WikiText-2 with LoRA\n\n## Model Description\n\nThis is a **GPT-2 Medium** (354M parameters) model fine-tuned on the **WikiText-2** dataset using **LoRA (Low-Rank Adaptation)**.\n\n- **Base Model:** gpt2-medium\n- **Fine-tuning Method:** LoRA (r=16, alpha=32)\n- **Dataset:** WikiText-2 (23,767 training samples)\n- **Training Time:** 1.81 hours on 2x Tesla T4 GPUs\n- **Final Validation Perplexity:** 20.73\n\n## Training Configuration\n```yaml\nLoRA Configuration:\n  - Rank (r): 16\n  - Alpha: 32\n  - Dropout: 0.05\n  - Target Modules: c_attn, c_proj, c_fc\n  - Trainable Parameters: 6.29M (1.74%)\n\nTraining Hyperparameters:\n  - Learning Rate: 3e-4\n  - Scheduler: Cosine\n  - Batch Size: 16 per GPU\n  - Gradient Accumulation: 4 steps\n  - Effective Batch Size: 128\n  - Epochs: 5\n  - Mixed Precision: FP16\n```\n\n## Performance\n\n| Metric | Value |\n|--------|-------|\n| Validation Perplexity | 20.73 |\n| Training Loss | 2.96 |\n| Training Time | 1.81h |\n| GPU Memory | ~8GB per GPU |\n\n## Usage\n\n### Installation\n```bash\npip install transformers peft torch\n```\n\n### Loading the Model\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\nimport torch\n\n# Load base model\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"gpt2-medium\",\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\n# Load LoRA weights\nmodel = PeftModel.from_pretrained(\n    base_model,\n    \"{REPO_ID}\"\n)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n\n# Generate text\nprompt = \"The future of artificial intelligence\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\noutputs = model.generate(\n    **inputs,\n    max_length=100,\n    temperature=0.8,\n    top_p=0.9,\n    do_sample=True\n)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\n### Merging LoRA Weights (Optional)\n\nFor faster inference, merge LoRA weights with base model:\n```python\n# Merge and save\nmerged_model = model.merge_and_unload()\nmerged_model.save_pretrained(\"./merged_model\")\ntokenizer.save_pretrained(\"./merged_model\")\n\n# Load merged model directly\nmodel = AutoModelForCausalLM.from_pretrained(\"./merged_model\")\n```\n\n## Training Details\n\n### Dataset\n\nWikiText-2 is a collection of high-quality articles from Wikipedia. The dataset contains:\n- Training: 23,767 samples\n- Validation: 2,461 samples  \n- Test: 2,891 samples\n\n### Training Procedure\n\n1. **Preprocessing:** Tokenization with max length 512\n2. **Optimization:** AdamW with fused implementation\n3. **Regularization:** Weight decay 0.01, gradient clipping 1.0\n4. **Learning Rate Schedule:** Cosine decay with 5% warmup\n5. **Early Stopping:** Patience of 3 evaluations\n\n### Training Curves\n\nThe model showed smooth convergence:\n\n{'loss': 3.4266, 'grad_norm': 0.375161349773407, 'learning_rate': 0.00029999620250019296, 'epoch': 0.2691790040376851}\n{'loss': 3.1084, 'grad_norm': 0.3222482204437256, 'learning_rate': 0.0002974401932923434, 'epoch': 0.5383580080753702}\n{'loss': 3.0748, 'grad_norm': 0.3670180141925812, 'learning_rate': 0.0002902305885868527, 'epoch': 0.8075370121130552}\n{'loss': 3.0336, 'grad_norm': 0.3346036374568939, 'learning_rate': 0.0002785949422362223, 'epoch': 1.0753701211305517}\n{'loss': 3.0089, 'grad_norm': 0.34160029888153076, 'learning_rate': 0.00026290050546456115, 'epoch': 1.3445491251682369}\n\n🎯 New Best PPL: 20.82\n\n{'eval_loss': 3.035845994949341, 'eval_runtime': 45.0469, 'eval_samples_per_second': 54.632, 'eval_steps_per_second': 0.866, 'epoch': 1.3445491251682369}\n\n{'loss': 2.9974, 'grad_norm': 0.32614269852638245, 'learning_rate': 0.00024364263546496366, 'epoch': 1.613728129205922}\n{'loss': 2.9802, 'grad_norm': 0.33632907271385193, 'learning_rate': 0.0002214291606397339, 'epoch': 1.8829071332436071}\n{'loss': 2.9432, 'grad_norm': 0.33555737137794495, 'learning_rate': 0.00019696119595708603, 'epoch': 2.1507402422611035}\n{'loss': 2.9274, 'grad_norm': 0.3619914650917053, 'learning_rate': 0.0001710110139414995, 'epoch': 2.4199192462987886}\n{'loss': 2.9213, 'grad_norm': 0.3930057883262634, 'learning_rate': 0.00014439766974675623, 'epoch': 2.6890982503364738}\n\n🎯 New Best PPL: 20.72\n\n{'eval_loss': 3.0311944484710693, 'eval_runtime': 44.8175, 'eval_samples_per_second': 54.912, 'eval_steps_per_second': 0.87, 'epoch': 2.6890982503364738}\n\n{'loss': 2.9254, 'grad_norm': 0.3837960958480835, 'learning_rate': 0.00011796114964767264, 'epoch': 2.958277254374159}\n{'loss': 2.8891, 'grad_norm': 0.3657948970794678, 'learning_rate': 9.253585889127956e-05, 'epoch': 3.2261103633916552}\n{'loss': 2.8775, 'grad_norm': 0.3783886134624481, 'learning_rate': 6.892428569973754e-05, 'epoch': 3.4952893674293404}\n{'loss': 2.8754, 'grad_norm': 0.39271751046180725, 'learning_rate': 4.787167265746529e-05, 'epoch': 3.7644683714670255}\n{'loss': 2.8685, 'grad_norm': 0.3550795614719391, 'learning_rate': 3.004249491929961e-05, 'epoch': 4.032301480484522}\n\n{'eval_loss': 3.031768560409546, 'eval_runtime': 44.9244, 'eval_samples_per_second': 54.781, 'eval_steps_per_second': 0.868, 'epoch': 4.032301480484522}\n\n{'loss': 2.8435, 'grad_norm': 0.38916313648223877, 'learning_rate': 1.599948764853796e-05, 'epoch': 4.301480484522207}\n{'loss': 2.858, 'grad_norm': 0.3778211176395416, 'learning_rate': 6.185884633398319e-06, 'epoch': 4.570659488559892}\n{'loss': 2.858, 'grad_norm': 0.36865052580833435, 'learning_rate': 9.11428677298881e-07, 'epoch': 4.839838492597577}\n\n{'train_runtime': 6566.6305, 'train_samples_per_second': 18.097, 'train_steps_per_second': 0.142, 'train_loss': 2.9640738128333965, 'epoch': 5.0}\n\n## Limitations\n\n- Fine-tuned on English Wikipedia text only\n- May not generalize well to other domains\n- LoRA adapters add small overhead during inference\n- Inherits biases from GPT-2 and Wikipedia\n\n## Intended Use\n\nThis model is intended for:\n- Text generation experiments\n- Research on parameter-efficient fine-tuning\n- Educational purposes\n- Transfer learning baselines\n\n## Citation\n\nIf you use this model, please cite:\n```bibtex\n@misc{{gpt2-wikitext2-lora,\n  author = {{Your Name}},\n  title = {{GPT-2 Medium Fine-tuned on WikiText-2 with LoRA}},\n  year = {{2025}},\n  publisher = {{HuggingFace}},\n  url = {{https://huggingface.co/{REPO_ID}}}\n}}\n```\n\n## Acknowledgments\n\n- Base model: OpenAI's GPT-2\n- LoRA: Microsoft Research\n- Training: Kaggle Tesla T4 GPUs\n- Framework: HuggingFace Transformers, PEFT\n\n## Contact\n\nFor questions or issues, please open an issue on the model repository.\n\"\"\"\n\n# Save model card\nwith open(\"README.md\", \"w\") as f:\n    f.write(model_card)\n\nprint(\"✓ Model card created: README.md\")\n\n# Upload model card\nfrom huggingface_hub import HfApi\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_API_TOKEN\")\n\napi = HfApi()\napi.upload_file(\n    path_or_fileobj=\"README.md\",\n    path_in_repo=\"README.md\",\n    repo_id=REPO_ID,\n    token=hf_token,\n    commit_message=\"Add model card\"\n)\n\nprint(f\"✓ Model card uploaded to: https://huggingface.co/{REPO_ID}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T08:00:02.028431Z","iopub.execute_input":"2025-10-28T08:00:02.029088Z","iopub.status.idle":"2025-10-28T08:00:02.727006Z","shell.execute_reply.started":"2025-10-28T08:00:02.029055Z","shell.execute_reply":"2025-10-28T08:00:02.726236Z"}},"outputs":[{"name":"stdout","text":"✓ Model card created: README.md\n✓ Model card uploaded to: https://huggingface.co/shiva9876/gpt2-medium-wikitext2-lora\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"### Trail and test after pushing on HUB","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\nbase_model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\")\nmodel = PeftModel.from_pretrained(\n    base_model,\n    \"shiva9876/gpt2-medium-wikitext2-lora\"\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"shiva9876/gpt2-medium-wikitext2-lora\")\n\nprompt = \"Once upon a time\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_length=50)\nprint(tokenizer.decode(outputs[0]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T08:09:51.326104Z","iopub.execute_input":"2025-10-28T08:09:51.326745Z","iopub.status.idle":"2025-10-28T08:10:03.496154Z","shell.execute_reply.started":"2025-10-28T08:09:51.326722Z","shell.execute_reply":"2025-10-28T08:10:03.495378Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43a30d215afe4387ad952a822fb41bc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/25.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"848a7e5ab9b146ac928fddacb459d626"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43133a3abb57424a9bee9a0798f0845d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0d0636278f844e8a6e93183df74c42c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a703620c6a20406ca8a8c5d42c2bce1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b668c61546ce4395a4d87468ccf786c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/131 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55e4db65071145ca8988921df67da0ec"}},"metadata":{}},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Once upon a time , in a land far away , there lived a king , called Odaenathus . He was a man of great wisdom , and he was a great warrior . He was also a man of great strength , and he was\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"### This below code can be used in Agentic RAG systems where this appoach would be very helpfull for the generation task after training on specific domain of dataset.","metadata":{}},{"cell_type":"code","source":"# Deploy with FastAPI\nfrom fastapi import FastAPI\nfrom peft import PeftModel\n\napp = FastAPI()\n\n# Load once at startup\nmodel = PeftModel.from_pretrained(\n    AutoModelForCausalLM.from_pretrained(\"gpt2-medium\"),\n    \"shiva_99/gpt2-medium-wikitext2-lora\"\n)\n\n@app.post(\"/generate\")\ndef generate(text: str):\n    # Use model\n    ...","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}