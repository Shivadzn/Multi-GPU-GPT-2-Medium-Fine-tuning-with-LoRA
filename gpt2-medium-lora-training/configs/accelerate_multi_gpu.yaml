# Multi-GPU configuration for Accelerate
compute_environment: LOCAL_MACHINE
distributed_type: MULTI_GPU
fp16: true
machine_rank: 0
main_process_ip: null
main_process_port: 29500
main_training_function: main
num_machines: 1
num_processes: 4  # Adjust based on number of GPUs
use_cpu: false

# Mixed precision training
mixed_precision: "fp16"

# Distributed training settings
downcast_bf16: false
dynamo_backend: 'NO'

ddp_bucket_cap_mb: 25
ddp_find_unused_parameters: false
ddp_timeout: 1800

# Logging and checkpointing
log_with: "tensorboard"
logging_dir: "../outputs/logs"

# Checkpointing
save_on_each_node: true

# Performance optimizations
dataloader_num_workers: 4
dataloader_pin_memory: true

# For debugging
debug: false

# For FSDP (optional, uncomment if using FSDP)
# fsdp_config:
#   fsdp_auto_wrap_policy: "TRANSFORMER_BASED_WRAP"
#   fsdp_backward_prefetch_policy: "BACKWARD_PRE"
#   fsdp_offload_params: false
#   fsdp_sharding_strategy: 1
#   fsdp_state_dict_type: "FULL_STATE_DICT"
#   fsdp_transformer_layer_cls_to_wrap: "GPT2Block"

# For DeepSpeed (optional, uncomment if using DeepSpeed)
# deepspeed_config:
#   gradient_accumulation_steps: 4
#   gradient_clipping: 1.0
#   offload_optimizer_device: "none"
#   offload_param_device: "none"
#   zero3_init_flag: false
#   zero_stage: 2

# For CPU offload (optional)
offload_optimizer_device: "none"
offload_param_device: "none"

# For gradient clipping
gradient_clipping: 1.0

# For tracking
track_memory: true

# For custom training loop (if needed)
use_ipex: false

# For CUDA graph (advanced)
use_cuda_graph: false

# For distributed training
gpu_ids: all  # Use all available GPUs

# For better performance with small models
# no_cuda: false

# For better memory efficiency
# fp16_opt_level: O1

# For better gradient checkpointing
# gradient_checkpointing: true

# For better logging
# report_to: "tensorboard"
# logging_steps: 100
# save_steps: 1000
# eval_steps: 1000

# For better reproducibility
# seed: 42

# For better memory management
torch_compile: false
