# Inherits from default config with WikiText-2 specific settings
_base_: default_config.yaml

data:
  dataset_name: "wikitext"
  dataset_config_name: "wikitext-2-raw-v1"
  max_seq_length: 256
  validation_split_percentage: 5

training:
  num_train_epochs: 5
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  learning_rate: 3.0e-5
  warmup_ratio: 0.1

model:
  model_name_or_path: "gpt2-medium"
  use_lora: true
  lora_rank: 8
  lora_alpha: 32
  lora_dropout: 0.1
